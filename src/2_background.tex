\chapter{背景}
\label{background}

本章では本研究の背景と関連する技術について概説する．

\section{惑星規模の分散システム}
\label{bg:definition}

本節では，本研究が対象とする惑星規模の分散システムの定義を明らかにする．
定義を分かりやすくするため，先にモノリスと分散システムについて概説し，違いを明らかにした上で惑星規模の分散システムを定義する．

\subsection{モノリス}
\label{bg:definition:monolith}

モノリスとは，モノシリックなシステムを指す．
モノリスは英語で一枚岩を意味し，システムにおいては，大きな単一のプログラムによって特定の処理を実行するアーキテクチャといえる．
このように，本来では大量のコードによって成り立つという意味を含むが，比較化のため本研究では，プログラムの大きさに関わらず単一のコンポーネントで構成されたシステムをモノリスと呼ぶことにした．
単一のコンポーネントで構成されるシステムはアーキテクチャがシンプルなため，システム自体が小さい場合は取り扱いやすい．
しかし，システムが大きくなるにつれて機能同士の依存関係が密な状態になることで細かい粒度でテストを行えなかったり，チーム開発における並行作業が困難になる傾向にある．

\subsection{分散システム}
\label{bg:definition:distributed-system}

分散システムとは，複数のコンポーネントが協調動作することによって動作するシステムを指す．
システム全体が複数の独立したコンポーネントを組み合わせて成り立っている点において，モノリスとは対照的である．
各コンポーネントは異なる役割を持っており，お互いに協調動作することによってシステム全体が動作する．
本研究で使用するKubernetesも，コンテナオーケストレーションに必要な機能をコンポーネント毎に分割した分散システムである．
Kubernetesに関する説明は~\ref{background:container-orchestration-system:kubernetes}章で詳しく行う．
分散システムは複雑で取り扱いづらいように思われるが，実際には機能が細分化され各コンポーネントの役割が明確になる．
機能同士の依存関係が希薄になるため細かい粒度でのテストが可能となり，障害時の原因特定も容易になる．
チーム開発において開発者同士で同じ箇所を担当する可能性も下がるため，開発スピードが向上するというメリットもある．

\subsection{惑星規模の分散システム}
\label{bg:definition:planetary-scale-distributed-system}

本研究で対象とする惑星規模の分散システムとは，~\ref{definition:distributed-system}の中でも世界中に地理的に分散したコンピュータが協調動作することによって成り立つシステムを指す．
近年注目を集めているブロックチェーンや2000年代初頭に登場したWinnyといったサービスが，惑星規模の分散システムにあたる．
これらのシステムに使用されている技術や，サービスについての概説は~\ref{bg:planetary-scale-distributed-system}にて行う．
惑星規模の分散システムは，システムを構成するコンピュータの場所を開発者が固定できないことと，システム全体がスケーリングしていく点で~\ref{bg:definition:distributed-system}章の分散システムとは異なる．
開発者は，コンピュータの位置やスケーリングした際の挙動を考慮した上で開発を行わなければならない．
よって，システムの挙動を左右する条件が~\ref{bg:definition:monolith}章のモノリスや~\ref{bg:definition:distributed-system}章の分散システムに比べて多くテストが難しい．

\section{惑星規模の分散システムにおける使用技術と参考例}
\label{bg:planetary-scale-distributed-system}

本節では，~\ref{bg:definition:planetary-scale-distributed-system}章で概説した惑星規模の分散システムの主な使用技術とサービスの参考例について概説する．
使用技術としては，P2Pが一番にあげられる．
クライアントサーバモデルとは異なるシステムモデルであり，最近ではブロックチェーン技術にも取り入れられている．
サービスの参考例では，WinnyやGnutella，Bitcoinについて触れる．
Bitcoinは，先述したブロックチェーン技術を用いて動作するシステムであり，仮想通貨として広く世間に認知されている．

\subsection{P2P}
\label{bg:planetary-scale-distributed-system:p2p}

P2Pは``Peer to Peer''の略である．
クライアントサーバモデルのシステムのように中央集権的な役割を担うサーバを必要とせず，コンピュータ同士が対等な関係を築く主従関係のなるシステムモデルである．

クライアントサーバモデルでは，クライアントがリクエストを投げサーバがレスポンスを返すという明確な役割分担がある．
サーバはクライアントからのリクエストを待ち，リクエストが来たときのみ必要な処理を行ってクライアントへレスポンスを返す．
対してクライアントは，サーバに問い合わせる必要がないときは何もせず，データを要求したり変更する必要が生じたときのみサーバとの通信を行う．
よって通信は常にクライアントが起点となり，基本的にサーバ起点の通信は行われない．

対象的に，P2Pでは各コンピュータが対等な関係性を持つため，クライアントサーバモデルのような明確な役割分担がシステム上ない．
P2Pでは各コンピュータが状況に応じてサーバとクライアントの役割を担う．
各コンピュータは臨機応変にサーバとしてレスポンスを返し，クライアントとしてリクエストを投げる動的システムが特徴としてあげられる．

クライアントサーバモデルでは，リクエストを発信する側をクライアント，それに対してレスポンスを返す側をサーバと呼んでいる．
対してP2Pでは，前述した通り各コンピュータは動的に役割を変化させサーバとしてもクライアントとしても動くことからサーバントと呼ばれる．
単にノードと呼ばれることもある．

\subsubsection{P2Pの特徴}

P2Pでは各コンピュータがサーバにもクライアントにも成り得るため，クライアントサーバモデルとは内部の実装も異なる．

まず第一に，データを保持する中央集権的なサーバが存在しないためアプリケーション上で必要になるデータは各コンピュータが保持する．
アプリケーションの実装方式によっても異なるが，各コンピュータがデータを分割して保持する場合もあれば全てのコンピュータが同じデータを保持する場合もある．
例えば，ブロックチェーンでは各コンピュータが全てのデータを保持しており（全てのデータを持たないタイプとしてシステムに参加することも可能），データを相互で検証し合うことによってデータの改竄耐性を向上させ，堅牢性を担保している．
また，ファイル共有システムであるWinnyでは，各コンピュータが保持しているデータは異なるため，データを参照する際はどのコンピュータが目的のデータを保持しているか検索し対象となるサーバを決定してから通信を行うといった処理が必要となる．

次に，システムを動かすプログラムを各コンピュータが保持し動作させなければいけない点でもクライアントサーバモデルとは異なる．
クライアントサーバモデルでは，システムのメインプログラムの実行はサーバの役割であるため，サーバのみがプログラムを保持しておけば良い．
対して，各コンピュータが状況に応じて役割を変えるP2Pではプログラムを各々で保持する必要性がある．
クライアントとして他のコンピュータが保持しているデータを参照したり，データを要求してきたコンピュータに対して応答をしなければならない．

\subsubsection{P2Pのメリット}

本節では，P2Pのメリットについて概説する．
P2Pシステムの利点としては，拡張性（スケーラビリティ）・耐障害性があげられる．

まず第一に拡張性に関しては，クライアントサーバモデルの場合，利用者が増大するとシステムの中心であるサーバへアクセスが集中し，サーバやその周辺のネットワークへの負荷が高くなり，システム的な弱点になる．
システム運用者は拡張性を高めるため，ネットワーク機器のスペックをあげたり，負荷が増大した際に自動でサーバの数を増やすオートスケーリングなどの対策を取らなければならない．
それに対してP2Pの場合，コンピュータ同士は相互に通信を行うためアクセスは分散されやすくなる．その点でP2Pは拡張性に長けている．

次に耐障害性である．クライアントサーバモデルの場合，何らかの原因でサーバが落ちるとサービス自体が停止してしまいサーバが構造上の単一障害点となる．
しかしP2Pではどこかのコンピュータが停止したとしても，正常なコンピュータ同士で新たなネットワークを形成することで問題なくサービスを継続することができる．
構造上の単一障害点が存在しないため，障害性に長けている．

\subsubsection{P2Pのデメリット}

本節では，P2Pのデメリットについて概説する．

第一に情報伝達における遅延があげられる．
P2Pでは接続先のコンピュータが固定ではないため，状況に応じて接続先を変更する必要がある．
すなわち，目的のデータを保持しているコンピュータを探し出したり，そもそもネットワーク上で近い距離に他コンピュータが存在しない場合，情報の取得や送信に遅延が生じてしまう．
全てのコンピュータで同じデータを保持するブロックチェーンのようなシステムにおいては，コンピュータ同士がバケツリレーのようにデータを受け渡さなければならず，端から端までデータを伝えるまでに時間が掛かってしまう問題点がある．

次にシステム全体での管理のしにくさがあげられる．
P2Pシステムでは各コンピュータでアプリケーションを動かすため，中央集権的なサーバと異なり，管理は各々のコンピュータ保持者に委ねられることになる．
よって，たとえシステムに問題点が見つかりアプリケーション開発者がパッチを含んだアップデートバージョンを配布した場合でも，実際に動かしているアプリケーションがアップデートされるかどうかは保証されない．
同様にシステム全体の監視を行うことも困難である．

\subsection{Winny}

Winnyはソフトウェアエンジニア金子勇氏が開発し，2002年に発表されたファイル共有ソフトである．
システム上で中央集権的なサーバを保持せず，ノード同士が相互に接続することで実現されるP2Pアプリケーションとして注目を浴びた．
ユーザはノード内に保持されたファイルを他のノードと共有することができるため，任意のファイルをアップロードしたり，逆に他のノードが保持しているファイルをダウンロードすることができる．
Winnyでは，受信ファイルの送信元や送信ファイルの宛先をユーザが確認することはできず，バックグラウンドでの処理はユーザに見せないよう高い秘匿性が担保されていた．
クライアントサーバモデルのシステムアーキテクチャとは打って変わって出た新しい形のアプリケーションであったが，高い匿名性も起因して，
一部のユーザが違法な音楽ファイルや動画ファイル，コンピュータウイルスをWinnyにアップロードしたことで著作権法違反が問われた．
開発者である金子氏にも疑いがかけられ2004年に逮捕，その後画期的な発明であったWinnyも衰退していった．

\subsection{Gnutella}

Winnyに同じくGnutellaも中央集権型サーバに依存せず，P2Pネットワーク上のノード間の通信のみでファイルを送受信を行うファイル共有アプリケーションである．

\subsection{Bitcoin}

Bitcoin~\cite{Bitcoin}は2008年にSatoshi Nakamotoと名乗る人物によって論文にて提唱されたものである．
2009年にはソフトウェアとして公開されており，今では多くのユーザに使用されている上，仮想通貨の先駆けとして他の仮想通貨を生む大きな起点となった．
同時に，2000年代後半に勢いを失っていたP2Pシステムの存在を再度世に知らしめ，開発の促進を促す起爆剤の役割を果たしたと考えられる．
Bitcoinは基盤技術のひとつとしてWinnyやGnutellaと共通するP2Pネットワークを採用している．
参加するノードはそれぞれがシステム上のデータを保持し相互にデータを検証しあうことで，第三者的監視機関を必要とせずにデータの堅牢性を担保することが可能である．

\section{ステージング環境}
\label{bg:staging}

本節では，本研究で着目するステージング環境について概説する．

ステージング環境とは，システムのテストを行うための環境である．
サービスを本番運用する環境と同じものをステージング環境として構築し，本番環境へデプロイする前に，開発したシステムが期待する動作を行うか確認する．
ステージング環境で不備を発見した場合，本番環境への適応はせずに開発環境にて修正を行う．
対してステージング環境でのシステムの正常な動作を確認できた場合は，本番環境へのデプロイ作業へ移行する．
開発環境と本番環境の間にステージング環境を挟むことで，サービスの予期せぬ不具合や軽微なバグ等を早期に発見できる．

以下，~\ref{bg:definition}章で概説したシステムのテスト方法ならびに必要となるステージング環境について，それぞれ説明を行う．

\subsection{モノリスの場合}
\label{bg:staging:monolith}

モノリスは，単一のコンポーネントで構成されるシステムである．
テスト方法は，システムに対して任意の入力を与えた際に，入力に対して期待する出力が行われるかどうかを確認すればよい．
具体的には，特定のURLに対してリクエストを送信した場合に期待するWebページが出力されるか，またはWebページのボタンを押した際にDBに対して期待する値が書き込まれるかなどである．
システム内のコンポーネントはひとつであるため，テスト対象もひとつに限られる．
ステージング環境の構築時には，モノリスなコンポーネントを用意すればよい．
サーバを用意する場所については本番環境に合わせればよいため，自社サーバを用いる場合はオンプレ環境に，クラウド環境を用いる場合は任意のクラウドサービスを使用してサーバを準備すればよい．

\subsection{分散システムの場合}
\label{bg:staging:distributed-system}

分散システムは，モノリスとは異なり，複数のコンポーネントから成り立つシステムである．
分散システムのテストを行う場合，各コンポーネントに関してはモノリスと同じく，任意の入力に対して期待する出力が返されるかを確認すればよい．
しかしモノシリックなシステムとは違う点として，コンポーネント同士の協調動作が正常に働いているかどうかを確認する必要がある．
各コンポーネントが正常に動いた場合でも，システム全体が正常な状態にあるとは限らないからである．
例えば，システム内の特定のコンポーネントに障害が発生した場合，システム全体としてはエラーを返して障害が発生したことを明らかにしなければならない．
処理の巻き戻しや停止を行う必要がある場合もある．
このような複数コンポーネントを跨いだ処理は，一連の流れを通した上で確認する必要があり，各コンポーネントのテストでは不十分である．
一方，ステージング環境の構築においてはモノリスと大きな違いはない．
本番環境を想定した場所にサーバを用意し，システムの動作に必要なすべてのコンポーネントを準備すればよい．

\subsection{惑星規模の分散システム}
\label{bg:staging:planetary-scale-distributed-system}

惑星規模の分散システムは，分散システム同様複数のコンポーネントで構成されるシステムである．
独立したコンポーネントがお互いに協調動作することによってシステム全体が動いている．
~\ref{bg:staging:distributed-system}の分散システム同様，各コンポーネントでのテストとシステム全体でのテストを行えば，システムが正常に動作することを保証できる．
しかし，惑星規模の分散システムのテストにおいて各コンポーネントのテストは他のシステム同様に行えるが，システム全体のテストは容易ではない．
何故なら，惑星規模の分散システムでは各コンピュータ（コンポーネント）は地理的に分散しており，ステージング環境の構築では地理的な場所を指定し分散させてサーバを配置したいからである．
モノリスや分散システムの場合，サーバの配置は開発者が任意に設定できるため，本番環境と同じ環境を容易に再現できたが，システムの構成要素の配置と数を固定できない惑星規模の分散システムにおいては再現が難しい．
とはいえ無限にスケーリングする可能性のある惑星規模の分散システムを完璧に再現することは不可能であるため，地理的に分散させた幾つかのサーバによって構成されるステージング環境を構築する必要があると考える．
実際に地理的に分散させたサーバによって成り立つステージング環境上で，コンポーネント同士の協調動作が正しく働いていることを確認できれば，対象の場所においてのシステムの正常性が担保される．

\section{コンテナオーケストレーションシステム}
\label{background:container-orchestration-system}

コンテナオーケストレーションシステムは，コンテナ型仮想環境を統合管理するためのプラットフォームおよびツールを指す．
2010年代半ばから脚光を浴びるようになり，今では世界的に数々のプロジェクトで本番環境に適用されている．
サービスの立ち上げや運用過程において必要となる機能が数多く搭載されており，開発者は素早くかつ効率的に開発を進められる．
コンテナはVirtual Machine（以下，VM）のデメリットを考慮して作られており，今後VMの代わりを担う次世代の技術としてより一層注目されていく技術であると考える．

本研究では，コンテナオーケストレーションシステムとしてKubernetesを，CRI（コンテナ・ランタイム・インターフェース）としてDockerを使用した．
本節では，コンテナおよびコンテナオーケストレーションの概説と実際に使用したKubernetesやDockerといったツールについて紹介する．

\subsection{コンテナ}
\label{background:container-orchestration-system:container}

本節では，コンテナおよびDockerについて概説する．

コンテナ型仮想化は，ひとつのコンピュータ上で仮想的に別のコンピュータを動作させる技術である．
ホストOSの上で動いている別のコンピュータをひとつひとつをコンテナと呼ぶ．

コンテナについて説明するにあたり，VMや物理マシンと比較しながら特徴を示していく．
コンテナは，挙動としてはVMと似ており，どちらも同じ課題を解決している．
VMが登場する前，開発者はひとつのサーバ上で複数のアプリケーションを動作させることに頭を悩ませていた．
何故なら，アプリケーションのうちのひとつがサーバのリソースを大幅に占有した場合，他のアプリケーションのパフォーマンスが低下してしまうからである．
解決策のひとつとして，アプリケーション毎に別々のサーバ上で動作させるものがあったが，デメリットとして維持費が嵩むことと使用されない無駄なリソースが生まれてしまうことがあった．
これを解決するために開発されたのがVMである．
VMはソフトウェアによって仮想的に物理マシンを実現する技術であり，ひとつの物理マシンCPU上で複数のVMを動作させることが可能である．
アプリケーションはそれぞれ独立しておりお互いに不可侵な関係性であるため，ひとつのアプリケーションがリソースを占有することはなく，よりリソースを効率的に使用できる．
スケーラビリティにも長けており，開発者はいつでもアプリを追加・削除でき，ハードウェアコストの削減にも貢献している．
しかし，VMは処理におけるオーバーヘッドが大きく起動時間が長いなどデメリットも存在する．
VMの後に登場した技術がコンテナ型仮想化である．
コンテナ型仮想化では，各アプリケーションはひとつのホストOSを共有するため，VMより軽量で起動時間も短い．
コンテナはコンテナイメージから作成され，イメージは宣言的なファイルに基づいて生成される．
これによって開発者はより簡単かつスピーディに開発を進めることが可能である．
"Build Once, Run Anywhere"というコンセプトが掲げられており，一度生成されたイメージはどの環境でも動作し冪等性が担保される．
一方，ホストOSを共有するためセキュリティ面では課題が見られる．

コンテナ仮想環境を構築するためのランタイムであるCRIには，dockershim（Docker）, containerd, cri-o, Frakti, rktlet（rkt）などが挙げられる．
本研究では,CRIのデファクトスタンダードであるDockerを採用している．

\subsubsection{Docker}
\label{background:container-orchestration-system:container:docker}

Docker~\cite{Docker}はコンテナ型仮想環境を実現するためのプラットフォームおよびツールである．
前述したようにDockerでは，宣言的なファイルから生成したコンテナイメージを元にコンテナを起動する．
設計書となる宣言的なファイルはDockerファイルと呼ばれる．
Dockerファイルでは，ベースとなるイメージをインポートしたり，特定のコマンドの実行やファイルのコピーを行うためのコマンドが提供されている．
ミドルウェアや各種環境設定をコード化して管理することができ（Infrastructure as Code），別の環境で何度実行しても同じ結果が保証される.
Dockerイメージをバージョン毎に管理するためのDocker Hubというサービスがあり，開発者は自身のレポジトリにイメージをプッシュしたり，他のレポジトリからイメージを取得することも可能である．

\subsection{Kubernetes}
\label{background:container-orchestration-system:kubernetes}

Kubernetes~\cite{Kubernetes}はコンテナオーケストレーションエンジンであり，コンテナ化されたアプリケーションのデプロイやスケーリングなどの管理を自動化するためのプラットフォームである．

もともとGoogle社内で利用されていたコンテナクラスタマネージャの「Borg」を基盤にして作られたオープンソースソフトウェアであるため信頼性が高く，現時点でコンテナオーケストレーションシステムのデファクトスタンダードとなっている．
Kubernetesでは，複数のKubernetes Nodeの管理やコンテナのローリングアップデート，オートスケーリング，死活監視，ログ管理などサービスを本番環境で動かす上で必要不可欠となる機能を備えている．
Docker同様，デプロイするコンテナとその周辺のリソースはYAML形式やJSON形式で記述した宣言的なコードによって管理する．
Infrastructure as Codeに則っているため，実行環境に左右されず毎回常に同じコンテナが起動される．

GCPを筆頭にクラウド環境でもサポートされるようになり，現時点でAWSとAzureにおいても提供されている．
そのためKubernetesは徐々に注目を集めるようになり，今では多くの企業の本番環境で取り入れられている．

Kubernetesは，複数のサーバを束ねたクラスタの上で動作する．
サーバの役割は二つに分かれており，システム全体を統合管理するサーバをマスターノード（コントロールプレーン），実際にコンテナを起動させるサーバをワーカーノードと呼ぶ．
マスターノードはシングルでも動作するが，基本的には冗長性や耐障害性を考慮して複数のマスターノードをクラスタリングすることが多い．
クラウド環境を用いた場合，クリックひとつでKubernetesクラスタを用意することができる．
状況に応じてワーカーノードを追加・削除でき，自由にスケーリング出来る点も強みである．
クラウドの種類によっては特定の条件に合わせて自動でノードのオートスケーリングを行うこともできるが，オンプレ環境では自前で実装する必要がある．

Kubernetes自体は，多数のコンポーネントによって構成されるマイクロサービスアーキテクチャを採用している．
すべてのコンポーネントがkube-apiserverと呼ばれるKubernetes内のAPIサーバを中心として動作しており，殆ど全ての処理はkube-apiserverを通して実行される．
kube-apiserverはマスターノードに含まれる．
他にもマスターノード内で動作するコンポーネントとしては，
Kubernetesクラスタのすべての情報を保持するetcd,
コンテナを起動させるノードをスケジューリングするkube-scheduler,
ノード上で動作するコンテナを監視し必要に応じてコンテナを追加・削除するよう指示するkube-controller-managerなどが挙げられる．
対してワーカーノードで動作する主なコンポーネントには,kubeletなどがある．
kubeletを含め，本研究の実装で用いたkubeadm,kubectlに関しては以下で詳細に説明する．

\subsubsection{Kubeadm}
\label{background:container-orchestration-system:kubernetes:kubeadm}

Kubeadm~\cite{Kubeadm}は，Kubenetesクラスタを構築するためのベストプラクティスを提供するツールである．
Kubeadmが提供するコマンドをいくつか以下に示す．\\*

{\bf kubeadm init}\\
クラスタの最初のコントロールプレーンとなるノードを起動する．\\*

{\bf kubeadm join}\\
クラスタに追加のコントロールプレーンまたはワーカーノードを参加させる．\\*

{\bf kubeadm upgrade}\\
クラスタのバージョンを最新へアップグレードする．\\*

{\bf kubeadm reset}\\
kubeadm initやkubeadm joinによって生じた変更を取り消す．\\*

\subsubsection{kubelet}
\label{background:container-orchestration-system:kubernetes:kubelet}

kubelet~\cite{kubelet}は，Kubernetesクラスタ内の各ワーカーノードで動作するコンポーネントである．
kubectlは，DockerなどのCRIと連携して実際にコンテナを起動・停止する役割をもつ．
具体的にはetcdの情報を監視して，自身のノードに割り当てられてまだ起動していないコンテナがあれば起動する．
etcdに格納された情報は，kube-apiserverやkube-controller-managerによってkube-apiserverを通して書き換えられ，実際のコンテナの操作に関してはkubeletが担うといった役割分担がされている．
~\ref{background:container-orchestration-system:kubernetes:kubeadm}章のkubeadm，ならびに~\ref{background:container-orchestration-system:kubernetes:kubectl}章のkubectlは，Kubernetesクラスタ構築時や操作時に用いるコマンドツールであるのに対して，
kubeletはコンテナの管理を行うデーモンとして動作する．

\subsubsection{kubectl}
\label{background:container-orchestration-system:kubernetes:kubectl}

kubectl~\cite{kubectl}は，Kubernetesクラスタをコントロールするためのツールである．
新規コンテナのデプロイや削除，アップデートから，動作中のコンテナやクラスタを構成するノードの情報の取得など，サービスの
運用を支援するAPIが提供されている．
kubectlが提供するコマンドをいくつか以下に示す．\\*

{\bf kubectl get nodes}\\
クラスタに参加するノードのステータスやロール（役割），IPアドレス等を取得する．\\*

{\bf kubectl get pods}\\
ポッドの名前やステータス，再起動の回数等を取得する．\\*

{\bf kubectl apply}\\
ポッドに新たな設定を反映させる．\\*

\section{OpenVPN}
\label{background:openvpn}

本節では、本研究で使用したVPN技術ならびにソフトウェアVPNであるOpenVPNについて概説する。

VPNとは、``Virtual Private Network''の略で，日本語では``仮想専用線''と呼ばれる．
VPNは，インターネット上に擬似的なプライベートネットワークを実現する技術，またはそのネットワーク自体を指す．
VPNを使用することで、インターネット上の異なるセグメント同士であっても、あたかも専用線で接続されているかのように通信することが可能である。
VPNにはL2VPNとL3VPNがあり、L2VPNは異なるセグメント同士を接続しひとつの擬似的なLANを構築するものであり、L3VPNではIPプロトコルでの通信が可能となる。
セキュリティ面においては、通信内容をカプセル化することでパケットの中身の覗き見や改竄のリスクを低減することができる。

\subsection{OpenVPN}

OpenVPN~\cite{OpenVPN}は、OpenVPN Technologies, inc.が中心になって開発しているオープンソースのVPNソフトウェアである．

OpenVPNは幅広いOSでサポートされており、Window, Linux, Mac OS, iOS, Androidで利用可能である。
異なるOS間でも利用可能であるため、ひとつのVPNネットワーク内に異なるOSが混在していても正常に動作する。
OpenVPNでは、対応したOSのサーバがひとつでもあれば簡単にVPNサーバを構築することが可能である。
VPNネットワークに参加するためには認証が必要であり、OpenVPNでは静的鍵による認証や証明書認証，ID/パスワード認証，二要素認証といった複数の認証方法から任意のものを選択できる。
接続方法としては、ルーティングとブリッジが提供され、ルーティングはL3VPN, ブリッジはL2VPNに対応する。
VPNネットワーク内でブロードキャストを行いたい場合など、擬似的なLANを構築したい場合以外は基本的にL3VPNを用いる。
L3VPNにあたるルーティングでは、クライアントサーバ接続とサイト間接続が提供されている。
クライアントサーバ接続では、各クライアントに認証設定が必要となり、接続の準備としてサーバ側での認証情報の生成が必要となる。
認証情報を共有後、クライアント側では接続のために設定ファイルを用意し、コマンドやアプリケーションを用いて接続を行う。
対するサイト間接続では、VPNの設定は各セグメントのVPNサーバで完結する。
VPNサーバ間での接続が確立できれば、各拠点に配置されたサーバはお互いに疎通が可能となる。
本研究の提案手法においては、ルーティング形式のサイト間接続を活用した。