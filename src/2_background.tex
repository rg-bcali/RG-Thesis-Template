\chapter{背景}
\label{background}

本章では本研究の背景と関連する技術について概説する．

\section{惑星規模の分散システム}
\label{bg:definition}

本節では、本研究が対象とする惑星規模の分散システムの定義を明らかにする。
定義を分かりやすくするため、先にモノリスと分散システムについて概説し、違いを明らかにした上で惑星規模の分散システムを定義する。

\subsection{モノリス}
\label{bg:definition:monolith}

モノリスとは、モノシリックなシステムを指す。
モノリスは英語で一枚岩を意味し、システムにおいては、大きな単一のプログラムによって特定の処理を実行するアーキテクチャといえる。
このように、本来では大量のコードによって成り立つという意味を含むが、比較化のため本研究では、プログラムの大きさに関わらず単一のコンポーネントで構成されたシステムをモノリスと呼ぶことにした。
単一のコンポーネントで構成されるシステムはアーキテクチャがシンプルなため、システム自体が小さい場合は取り扱いやすい。
しかし、システムが大きくなるにつれて機能同士の依存関係が密な状態になることで細かい粒度でテストを行えなかったり、チーム開発における並行作業が困難になる傾向にある。

\subsection{分散システム}
\label{bg:definition:distributed-system}

分散システムとは、マイクロサービスアーキテクチャなシステムを指す。
システム全体が複数の独立したコンポーネントを組み合わせて成り立っている点において、モノリスとは対照的である。
各コンポーネントは異なる役割を持っており、お互いに協調動作することによってシステム全体が動作する。
本研究で使用するKubernetesも、コンテナオーケストレーションに必要な機能をコンポーネント毎に分割した分散システムである。
Kubernetesに関する説明は~\ref{background:container-orchestration-system:kubernetes}章で詳しく行う。
分散システムは複雑で取り扱いづらいように思われるが、実際には機能が細分化され各コンポーネントの役割が明確になる。
機能同士の依存関係が希薄になるため細かい粒度でのテストが可能となり、障害時の原因特定も容易になる。
チーム開発において開発者同士で同じ箇所を担当する可能性も下がるため、開発スピードが向上するというメリットもある。

\subsection{惑星規模の分散システム}
\label{bg:definition:planetary-scale-distributed-system}

本研究で対象とする惑星規模の分散システムとは、~\ref{definition:distributed-system}の中でも世界中に地理的に分散したコンピュータが協調動作することによって成り立つシステムを指す。
近年注目を集めているブロックチェーンや2000年代初頭に登場したWinnyといったサービスが、惑星規模の分散システムにあたる。
これらのシステムに使用されている技術や、サービスについての概説は~\ref{bg:planetary-scale-distributed-system}にて行う。
惑星規模の分散システムは、システムを構成するコンピュータの場所を開発者が固定できないことと、システム全体がスケーリングしていく点で~\ref{bg:definition:distributed-system}章の分散システムとは異なる。
開発者は、コンピュータの位置やスケーリングした際の挙動を考慮した上で開発を行わなければならない。
よって、システムの挙動を左右する条件が~\ref{bg:definition:monolith}章のモノリスや~\ref{bg:definition:distributed-system}章の分散システムに比べて多くテストが難しい。

\section{惑星規模の分散システムにおける使用技術と参考例}
\label{bg:planetary-scale-distributed-system}

本節では、~\ref{bg:definition:planetary-scale-distributed-system}章で概説した惑星規模の分散システムの主な使用技術とサービスの参考例について概説する。
使用技術としては、P2Pが一番にあげられる。
クライアントサーバモデルとは異なるシステムモデルであり、最近ではブロックチェーン技術にも取り入れられている。
サービスの参考例では、WinnyやGnutella、Bitcoinについて触れる。
Bitcoinは、先述したブロックチェーン技術を用いて動作するシステムであり、仮想通貨として広く世間に認知されている。

\subsection{P2P}
\label{bg:planetary-scale-distributed-system:p2p}

P2Pは``Peer to Peer''の略である。
クライアントサーバモデルのシステムのように中央集権的な役割を担うサーバを必要とせず、コンピュータ同士が対等な関係を築く主従関係のなるシステムモデルである。

クライアントサーバモデルでは，クライアントがリクエストを投げサーバがレスポンスを返すという明確な役割分担がある．
サーバはクライアントからのリクエストを待ち，リクエストが来たときのみ必要な処理を行ってクライアントへレスポンスを返す．
対してクライアントは、サーバに問い合わせる必要がないときは何もせず，データを要求したり変更する必要が生じたときのみサーバとの通信を行う．
よって通信は常にクライアントが起点となり，基本的にサーバ起点の通信は行われない．

対象的に，P2Pでは各コンピュータが対等な関係性を持つため，クライアントサーバモデルのような明確な役割分担がシステム上ない．
P2Pでは各コンピュータが状況に応じてサーバとクライアントの役割を担う．
各コンピュータは臨機応変にサーバとしてレスポンスを返し，クライアントとしてリクエストを投げる動的システムが特徴としてあげられる．

クライアントサーバモデルでは，リクエストを発信する側をクライアント，それに対してレスポンスを返す側をサーバと呼んでいる。
対してP2Pでは、前述した通り各コンピュータは動的に役割を変化させサーバとしてもクライアントとしても動くことからサーバントと呼ばれる．
単にノードと呼ばれることもある．

\subsection{P2Pの特徴}

P2Pでは各コンピュータがサーバにもクライアントにも成り得るため，クライアントサーバモデルとは内部の実装も異なる．

まず第一に，データを保持する中央集権的なサーバが存在しないためアプリケーション上で必要になるデータは各コンピュータが保持する．
アプリケーションの実装方式によっても異なるが，各コンピュータがデータを分割して保持する場合もあれば全てのコンピュータが同じデータを保持する場合もある．
例えば、ブロックチェーンでは各コンピュータが全てのデータを保持しており（全てのデータを持たないタイプとしてシステムに参加することも可能），データを相互で検証し合うことによってデータの改竄耐性を向上させ，堅牢性を担保している．
また，ファイル共有システムであるWinnyでは，各コンピュータが保持しているデータは異なるため，データを参照する際はどのコンピュータが目的のデータを保持しているか検索し対象となるサーバを決定してから通信を行うといった処理が必要となる．

次に，システムを動かすプログラムを各コンピュータが保持し動作させなければいけない点でもクライアントサーバモデルとは異なる．
クライアントサーバモデルでは、システムのメインプログラムの実行はサーバの役割であるため、サーバのみがプログラムを保持しておけば良い。
対して，各コンピュータが状況に応じて役割を変えるP2Pではプログラムを各々で保持する必要性がある．
クライアントとして他のコンピュータが保持しているデータを参照したり，データを要求してきたコンピュータに対して応答をしなければならない。

\subsection{P2Pのメリット}

本節では，P2Pのメリットについて概説する．
P2Pシステムの利点としては，拡張性（スケーラビリティ）・耐障害性があげられる．

まず第一に拡張性に関しては，クライアントサーバモデルの場合、利用者が増大するとシステムの中心であるサーバへアクセスが集中し，サーバやその周辺のネットワークへの負荷が高くなり，システム的な弱点になる．
システム運用者は拡張性を高めるため，ネットワーク機器のスペックをあげたり，負荷が増大した際に自動でサーバの数を増やすオートスケーリングなどの対策を取らなければならない．
それに対してP2Pの場合，コンピュータ同士は相互に通信を行うためアクセスは分散されやすくなる．その点でP2Pは拡張性に長けている．

次に耐障害性である．クライアントサーバモデルの場合，何らかの原因でサーバが落ちるとサービス自体が停止してしまいサーバが構造上の単一障害点となる．
しかしP2Pではどこかのコンピュータが停止したとしても，正常なコンピュータ同士で新たなネットワークを形成することで問題なくサービスを継続することができる。
構造上の単一障害点が存在しないため、障害性に長けている．

\subsection{P2Pのデメリット}

本節では，P2Pのデメリットについて概説する．

第一に情報伝達における遅延があげられる．
P2Pでは接続先のコンピュータが固定ではないため，状況に応じて接続先を変更する必要がある．
すなわち，目的のデータを保持しているコンピュータを探し出したり，そもそもネットワーク上で近い距離に他コンピュータが存在しない場合，情報の取得や送信に遅延が生じてしまう．
全てのコンピュータで同じデータを保持するブロックチェーンのようなシステムにおいては，コンピュータ同士がバケツリレーのようにデータを受け渡さなければならず，端から端までデータを伝えるまでに時間が掛かってしまう問題点がある．

次にシステム全体での管理のしにくさがあげられる。
P2Pシステムでは各コンピュータでアプリケーションを動かすため，中央集権的なサーバと異なり，管理は各々のコンピュータ保持者に委ねられることになる．
よって，たとえシステムに問題点が見つかりアプリケーション開発者がパッチを含んだアップデートバージョンを配布した場合でも，実際に動かしているアプリケーションがアップデートされるかどうかは保証されない．
同様にシステム全体の監視を行うことも困難である．

\section{惑星規模の分散システム}

\subsection{Winny}

Winnyはソフトウェアエンジニア金子勇氏が開発し，2002年に発表されたファイル共有ソフトである．
システム上で中央集権的なサーバを保持せず，ノード同士が相互に接続することで実現されるP2Pアプリケーションとして注目を浴びた．
ユーザはノード内に保持されたファイルを他のノードと共有することができるため，任意のファイルをアップロードしたり，逆に他のノードが保持しているファイルをダウンロードすることができる．
Winnyでは，受信ファイルの送信元や送信ファイルの宛先をユーザが確認することはできず，バックグラウンドでの処理はユーザに見せないよう高い秘匿性が担保されていた．
クライアントサーバモデルのシステムアーキテクチャとは打って変わって出た新しい形のアプリケーションであったが，高い匿名性も起因して，
一部のユーザが違法な音楽ファイルや動画ファイル，コンピュータウイルスをWinnyにアップロードしたことで著作権法違反が問われた．
開発者である金子氏にも疑いがかけられ2004年に逮捕，その後画期的な発明であったWinnyも衰退していった．

\subsection{Gnutella}

Winnyに同じくGnutellaも中央集権型サーバに依存せず，P2Pネットワーク上のノード間の通信のみでファイルを送受信を行うファイル共有アプリケーションである．

\subsection{Bitcoin}

Bitcoin~\cite{Bitcoin}は2008年にSatoshi Nakamotoと名乗る人物によって論文にて提唱されたものである．
2009年にはソフトウェアとして公開されており，今では多くのユーザに使用されている上，仮想通貨の先駆けとして他の仮想通貨を生む大きな起点となった．
同時に，2000年代後半に勢いを失っていたP2Pシステムの存在を再度世に知らしめ，開発の促進を促す起爆剤の役割を果たしたと考えられる．
Bitcoinは基盤技術のひとつとしてWinnyやGnutellaと共通するP2Pネットワークを採用している．
参加するノードはそれぞれがシステム上のデータを保持し相互にデータを検証しあうことで，第三者的監視機関を必要とせずにデータの堅牢性を担保することが可能である．

\section{ステージング環境}

\subsection{モノリスの場合}

\subsection{分散システムの場合}

\subsection{惑星規模の分散システム}

\section{地理的に分散したシステムとステージング環境での動作確認}
\label{background:staging-environment}
本節では，地理的に分散したシステムのステージング環境と動作確認について概説する．

\subsection{最小限の動作確認}
最も簡単に行える動作確認は，ふたつのノード間で行うテストである．ネットワーク上の二点でそれぞれノードを立ち上げ，システムの機能が正しく動作するかを確認する．
クライアントサーバモデルでは，最低限ではあるが機能の保証ができる．クライアントサーバモデルでは，中央集権的サーバとクライアントが一対一の関係で繋がっており，開発者はクライアントとの通信ただひとつに注力すればいいからだ．
ユーザが増加した場合の障害対策やレスポンスタイムの向上は確かに必要であるが，サーバとクライアントの一対一の関係性は不変であるため，ネットワーク自体が正常で有る限り問題は二点間に閉ざされておりテストがしやすい．

一方，中央集権的サーバがなくノード同士がサーバにもクライアントにもなり得る地理的に分散したシステムでは，この方法は十分ではない．ネットワークに参加するノードが増加すれば個々のノード同士の関係性は変化し，関係性が固定されないためである．
もうひとつの理由として，ノード周辺のネットワーク環境によって動作に影響が出る可能性が考えられる．地理的に分散したシステムの具体例として挙げたBitcoinでは，参加するノードは全て同じデータを保持する．
データの送信や受信において遅延が発生すれば何らかの影響が出ることは簡単に予想可能である．例えシステム上でデータの不整合を防ぐロジックが組まれていたとしても，ロジックを表現したコードが実際の環境で正常な動作をすることを動作確認無しで担保することは難しい．
以上の理由から，地理的に分散したシステムの動作確認をするにあたって二点間でのステージング環境は不十分であり，より多くのノードを実際の世界規模のネットワーク上で動かしたステージング環境が必要であると考えられる．

\subsection{地理的に分散したノードによる動作確認}
上記で述べた通り，地理的に分散したシステムのステージング環境は世界規模のネットワーク上で構築する必要性がある．
しかしこの方法は，ステージング環境の構築ならびに動作確認の進行において多大なるコミュニケーションコストとヒューマンリソースが予想される．
まず環境構築において地理的に離れた地点にノードを設置する必要性がある．地点ごとにノードを設置する人に加え，ノードのスペックやネットワークの構成等について共有するためのコミュニケーションが必要となる．
必要な物理筐体が揃ったのち，地理的に分散したシステム上で走らせるアプリケーションを各ポイントに配布し，各開発者は受け取ったアプリケーションファイルを設置したノードの上で走らせる必要がある．
ステージング環境でシステムを走らせた後に関しては，機能面や性能面での動作確認を行い，修正箇所があれば開発者がパッチを適応した後，修正後のアプリケーションファイルを各ポイントに配布するところから再度やり直さなければならない．
修正箇所が増加するに比例して，コミュニケーションコストと必要なヒューマンリソースは膨れ上がることが予想される．
さらにコミュニケーションの不足や伝達ミス等の人的ミスにより理想的な動作確認が行えないケースも考えられる．
以上の点から，地理的に分散したシステムのステージング環境においてコミュニケーションベースの動作確認には多くの課題があり現実的に困難である．
それ故，地理的に分散したノードを任意のポイントから統合的に管理することによって各地点での作業や地点間でのコミュニケーションを削減する必要性があると考えられる．

\subsection{独自実装のデバッグエージェントによる動作確認}
既存の提案として，地理的に分散したノードを統合管理・操作するために別アプリケーションを独自で開発する手法がある．
別アプリケーションとは，対象アプリケーションに対して命令を送信したり通信内容をログとして抽出するなどのデバッグエージェントして動作する．
ノードを統合管理出来る点では要件を満たしており，コミュニケーションならびに工数の削減に繋がると考えられる．
しかし対象アプリケーションにパッチを適用したい場合，同様にそれを操作するデバッグエージェントにも変更を加える必要があり，変更への弱さが窺える．
アップデートへの柔軟性が不足している限り，それによって生じるオーバーヘッドを削減することが出来ず根本的な解決に繋がらないと思われる．
分散したノードを一斉にコントロールだけでなく，アプリケーションの停止や更新といった変更においてもより少ない手間で抑えられることが求められ，
それを満たした際に地理的に分散したシステムの十分なステージング環境が成り立つと考えられる．

\section{コンテナオーケストレーションシステム}
\label{background:container-orchestration-system}

コンテナオーケストレーションシステムは、コンテナ型仮想環境を統合管理するためのプラットフォームおよびツールを指す。
2010年代半ばから脚光を浴びるようになり、今では世界的に数々のプロジェクトで本番環境に適用されている。
サービスの立ち上げや運用過程において必要となる機能が数多く搭載されており、開発者は素早くかつ効率的に開発を進められる。
コンテナはVirtual Machine（以下、VM）のデメリットを考慮して作られており、今後VMの代わりを担う次世代の技術としてより一層注目されていく技術であると考える。

本研究では、コンテナオーケストレーションシステムとしてKubernetesを、CRI（コンテナ・ランタイム・インターフェース）としてDockerを使用した。
本節では、コンテナおよびコンテナオーケストレーションの概説と実際に使用したKubernetesやDockerといったツールについて紹介する。

\subsection{コンテナ}
\label{background:container-orchestration-system:container}

本節では、コンテナおよびDockerについて概説する。

コンテナ型仮想化は、ひとつのコンピュータ上で仮想的に別のコンピュータを動作させる技術である。
ホストOSの上で動いている別のコンピュータをひとつひとつをコンテナと呼ぶ。

コンテナについて説明するにあたり、VMや物理マシンと比較しながら特徴を示していく。
コンテナは、挙動としてはVMと似ており、どちらも同じ課題を解決している。
VMが登場する前、開発者はひとつのサーバ上で複数のアプリケーションを動作させることに頭を悩ませていた。
何故なら、アプリケーションのうちのひとつがサーバのリソースを大幅に占有した場合、他のアプリケーションのパフォーマンスが低下してしまうからである。
解決策のひとつとして、アプリケーション毎に別々のサーバ上で動作させるものがあったが、デメリットとして維持費が嵩むことと使用されない無駄なリソースが生まれてしまうことがあった。
これを解決するために開発されたのがVMである。
VMはソフトウェアによって仮想的に物理マシンを実現する技術であり、ひとつの物理マシンCPU上で複数のVMを動作させることが可能である。
アプリケーションはそれぞれ独立しておりお互いに不可侵な関係性であるため、ひとつのアプリケーションがリソースを占有することはなく、よりリソースを効率的に使用できる。
スケーラビリティにも長けており、開発者はいつでもアプリを追加・削除でき、ハードウェアコストの削減にも貢献している。
しかし、VMは処理におけるオーバーヘッドが大きく起動時間が長いなどデメリットも存在する。
VMの後に登場した技術がコンテナ型仮想化である。
コンテナ型仮想化では、各アプリケーションはひとつのホストOSを共有するため、VMより軽量で起動時間も短い。
コンテナはコンテナイメージから作成され、イメージは宣言的なファイルに基づいて生成される。
これによって開発者はより簡単かつスピーディに開発を進めることが可能である。
"Build Once, Run Anywhere"というコンセプトが掲げられており、一度生成されたイメージはどの環境でも動作し冪等性が担保される。
一方、ホストOSを共有するためセキュリティ面では課題が見られる。

コンテナ仮想環境を構築するためのランタイムであるCRIには、dockershim（Docker）,containerd,cri-o,Frakti,rktlet（rkt）などが挙げられる。
本研究では,CRIのデファクトスタンダードであるDockerを採用している。

\subsubsection{Docker}
\label{background:container-orchestration-system:container:docker}

Docker~\cite{Docker}はコンテナ型仮想環境を実現するためのプラットフォームおよびツールである．
前述したようにDockerでは、宣言的なファイルから生成したコンテナイメージを元にコンテナを起動する。
設計書となる宣言的なファイルはDockerファイルと呼ばれる。
Dockerファイルでは、ベースとなるイメージをインポートしたり、特定のコマンドの実行やファイルのコピーを行うためのコマンドが提供されている。
ミドルウェアや各種環境設定をコード化して管理することができ（Infrastructure as Code）、別の環境で何度実行しても同じ結果が保証される.
Dockerイメージをバージョン毎に管理するためのDocker Hubというサービスがあり、開発者は自身のレポジトリにイメージをプッシュしたり、他のレポジトリからイメージを取得することも可能である。

\subsection{Kubernetes}
\label{background:container-orchestration-system:kubernetes}

Kubernetes~\cite{Kubernetes}はコンテナオーケストレーションエンジンであり，コンテナ化されたアプリケーションのデプロイやスケーリングなどの管理を自動化するためのプラットフォームである．

もともとGoogle社内で利用されていたコンテナクラスタマネージャの「Borg」を基盤にして作られたオープンソースソフトウェアであるため信頼性が高く、現時点でコンテナオーケストレーションシステムのデファクトスタンダードとなっている．
Kubernetesでは，複数のKubernetes Nodeの管理やコンテナのローリングアップデート，オートスケーリング，死活監視，ログ管理などサービスを本番環境で動かす上で必要不可欠となる機能を備えている．
Docker同様、デプロイするコンテナとその周辺のリソースはYAML形式やJSON形式で記述した宣言的なコードによって管理する。
Infrastructure as Codeに則っているため、実行環境に左右されず毎回常に同じコンテナが起動される。

GCPを筆頭にクラウド環境でもサポートされるようになり、現時点でAWSとAzureにおいても提供されている。
そのためKubernetesは徐々に注目を集めるようになり，今では多くの企業の本番環境で取り入れられている．

Kubernetesは、複数のサーバを束ねたクラスタの上で動作する。
サーバの役割は二つに分かれており、システム全体を統合管理するサーバをマスターノード（コントロールプレーン）、実際にコンテナを起動させるサーバをワーカーノードと呼ぶ。
マスターノードはシングルでも動作するが、基本的には冗長性や耐障害性を考慮して複数のマスターノードをクラスタリングすることが多い。
クラウド環境を用いた場合、クリックひとつでKubernetesクラスタを用意することができる。
状況に応じてワーカーノードを追加・削除でき、自由にスケーリング出来る点も強みである。
クラウドの種類によっては特定の条件に合わせて自動でノードのオートスケーリングを行うこともできるが、オンプレ環境では自前で実装する必要がある。

Kubernetes自体は、多数のコンポーネントによって構成されるマイクロサービスアーキテクチャを採用している。
すべてのコンポーネントがkube-apiserverと呼ばれるKubernetes内のAPIサーバを中心として動作しており、殆ど全ての処理はkube-apiserverを通して実行される。
kube-apiserverはマスターノードに含まれる。
他にもマスターノード内で動作するコンポーネントとしては、
Kubernetesクラスタのすべての情報を保持するetcd,
コンテナを起動させるノードをスケジューリングするkube-scheduler,
ノード上で動作するコンテナを監視し必要に応じてコンテナを追加・削除するよう指示するkube-controller-managerなどが挙げられる。
対してワーカーノードで動作する主なコンポーネントには,kubeletなどがある。
kubeletを含め、本研究の実装で用いたkubeadm,kubectlに関しては以下で詳細に説明する。


\subsubsection{Kubeadm}
\label{background:container-orchestration-system:kubernetes:kubeadm}

Kubeadm~\cite{Kubeadm}は，Kubenetesクラスタを構築するためのベストプラクティスを提供するツールである．
Kubeadmが提供するコマンドをいくつか以下に示す．\\*

{\bf kubeadm init}\\
クラスタの最初のコントロールプレーンとなるノードを起動する．\\*

{\bf kubeadm join}\\
クラスタに追加のコントロールプレーンまたはワーカーノードを参加させる．\\*

{\bf kubeadm upgrade}\\
クラスタのバージョンを最新へアップグレードする．\\*

{\bf kubeadm reset}\\
kubeadm initやkubeadm joinによって生じた変更を取り消す．\\*

\subsubsection{kubelet}
\label{background:container-orchestration-system:kubernetes:kubelet}

kubelet~\cite{kubelet}は，

\subsubsection{kubectl}
\label{background:container-orchestration-system:kubernetes:kubectl}

kubectl~\cite{kubectl}は，Kubernetesクラスタをコントロールするためのツールである．
新規コンテナのデプロイや削除，アップデートから，動作中のコンテナやクラスタを構成するノードの情報の取得など，サービスの
運用を支援するAPIが提供されている．
kubectlが提供するコマンドをいくつか以下に示す．\\*

{\bf kubectl get nodes}\\
クラスタに参加するノードのステータスやロール（役割），IPアドレス等を取得する．\\*

{\bf kubectl get pods}\\
ポッドの名前やステータス，再起動の回数等を取得する．\\*

{\bf kubectl apply}\\
ポッドに新たな設定を反映させる．\\*

\section{OpenVPN}
\label{background:openvpn}

\subsection{VPN}

VPNは``Virtual Private Network''の略で，日本語では``仮想専用線''と呼ばれる．
VPNは，パブリックネットワーク上で擬似的なプライベートネットワークを実現する技術，またはそのネットワークを指す．
トンネリング技術によって通信内容をカプセル化することで，パケットの中身の覗き見や改竄のリスクを提言することも可能である．

\subsection{OpenVPN}

OpenVPN~\cite{OpenVPN}はOpenVPN Technologies, inc.を中心に開発が行われているオープンソースのVirtual Private Networkソフトウェアである．
OpenVPNはWindowsやLinux，Mac OS，iOS, Androidでも利用でき，幅広いOS上で動作可能だ．
認証方法も豊富であり，静的鍵による認証や証明書認証，ID/パスワード認証，二要素認証をサポートしている．
VPNに関しても，マルチクライアントVPNに加えサイト間VPNの設定が可能であり，用途によって使い分けることができる．